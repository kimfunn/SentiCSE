{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_list = [\"stance_abortion\",\"stance_atheism\", \"stance_climate\",\"stance_feminist\", \"stance_hillary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tweet_eval 생성시\n",
    "# #task_name = stance_list[4] # \"emoji\"\n",
    "# task_name = \"emotion\"\n",
    "# num_label = 4\n",
    "# # label 별로 생성 상수, 2->1, 3->1.5 4->2\n",
    "# emoji_dataset = datasets.load_dataset(\"tweet_eval\", task_name)\n",
    "# emoji_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sst2생성시\n",
    "# num_label = 2\n",
    "# task_name=\"sst2\"\n",
    "# # label 별로 생성 상수, 2->1, 3->1.5 4->2\n",
    "# sst2 = load_dataset(task_name)\n",
    "# sst2 = sst2.remove_columns([\"idx\"])\n",
    "# sst2 = sst2.rename_column('sentence', 'text')\n",
    "# emoji_dataset = sst2\n",
    "# emoji_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uj-user/miniconda3/envs/simcse/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using custom data configuration SetFit--sst5-0c891184cb873f87\n",
      "Reusing dataset json (/home/uj-user/.cache/huggingface/datasets/SetFit___json/SetFit--sst5-0c891184cb873f87/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf86c676a834cd6871adb737cd6f01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8544\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2210\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # sst5생성시\n",
    "# num_label = 5\n",
    "# task_name=\"sst5\"\n",
    "# # label 별로 생성 상수, 2->1, 3->1.5 4->2\n",
    "# sst5 = load_dataset(\"SetFit/\"+task_name)\n",
    "# sst5 = sst5.remove_columns([\"label_text\"])\n",
    "# emoji_dataset = sst5\n",
    "# emoji_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset yelp_polarity (/home/uj-user/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/14f90415c754f47cf9087eadac25823a395fef4400c7903c5897f55cfaaa6f61)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4685c59aee124d59b28db70f5e408d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 560000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 38000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet_eval 생성시\n",
    "#task_name = stance_list[4] # \"emoji\"\n",
    "task_name = \"yelp_polarity\"\n",
    "num_label = 2\n",
    "# label 별로 생성 상수, 2->1, 3->1.5 4->2\n",
    "emoji_dataset = datasets.load_dataset(task_name)\n",
    "emoji_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/uj-user/SenCSE/utils/emoji_group.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B61.41.8.123/home/uj-user/SenCSE/utils/emoji_group.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# emoji_train = emoji_dataset['train'].to_pandas()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B61.41.8.123/home/uj-user/SenCSE/utils/emoji_group.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# emoji_valid = emoji_dataset['validation'].to_pandas()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B61.41.8.123/home/uj-user/SenCSE/utils/emoji_group.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# emoji_test = emoji_dataset['test'].to_pandas()\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B61.41.8.123/home/uj-user/SenCSE/utils/emoji_group.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m emoji_valid \u001b[39m=\u001b[39m emoji_dataset[\u001b[39m'\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mto_pandas()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B61.41.8.123/home/uj-user/SenCSE/utils/emoji_group.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m emoji_valid[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalue_counts()\n",
      "File \u001b[0;32m~/miniconda3/envs/simcse/lib/python3.8/site-packages/datasets/dataset_dict.py:50\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, k) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dataset:\n\u001b[1;32m     49\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(k, (\u001b[39mstr\u001b[39m, NamedSplit)) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 50\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(k)\n\u001b[1;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         available_suggested_splits \u001b[39m=\u001b[39m [\n\u001b[1;32m     53\u001b[0m             split \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m (Split\u001b[39m.\u001b[39mTRAIN, Split\u001b[39m.\u001b[39mTEST, Split\u001b[39m.\u001b[39mVALIDATION) \u001b[39mif\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m     54\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation'"
     ]
    }
   ],
   "source": [
    "# emoji_train = emoji_dataset['train'].to_pandas()\n",
    "# emoji_valid = emoji_dataset['validation'].to_pandas()\n",
    "# emoji_test = emoji_dataset['test'].to_pandas()\n",
    "emoji_valid = emoji_dataset['validation'].to_pandas()\n",
    "emoji_valid['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallest_label_num = sorted(emoji_valid['label'].value_counts().tolist())[0]\n",
    "smallest_label_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 생성함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_emoji_pair_list = []\n",
    "\n",
    "def make_same_pair(label_num, data_num, seed=2022):\n",
    "    global emoji_valid\n",
    "    # 1. 같은 레이블끼리의 데이터를 2개뽑아 1또는 2로 지정\n",
    "    for i in range(label_num): # 레이블별로 \n",
    "        for j in range(data_num): # 300개 추출\n",
    "            pair = emoji_valid[emoji_valid['label']==i].sample(2, random_state=seed)\n",
    "            emoji_valid = emoji_valid.drop(pair.index) # 추출된 데이터 삭제\n",
    "\n",
    "            pair_output = pair[\"text\"].tolist()\n",
    "            sts_emoji_pair_list.append([pair_output[0], pair_output[1], 1]) # 레이블1 데이터 추가\n",
    "\n",
    "def make_another_pair(label_num, data_num, seed=2022):\n",
    "    global emoji_valid\n",
    "    # 2. 각기 다른 레이블을 두개 뽑아서 0으로 지정\n",
    "    all_group = list(range(label_num))\n",
    "    a = 0\n",
    "    for a_label, b_label in list(combinations(all_group, 2)): # 20C2 = 190개의 조합 \n",
    "        for _ in range(data_num):\n",
    "            a_pair = emoji_valid[emoji_valid['label']==a_label].sample(1, random_state=seed)\n",
    "            b_pair = emoji_valid[emoji_valid['label']==b_label].sample(1, random_state=seed)\n",
    "            \n",
    "            emoji_valid = emoji_valid.drop([a_pair.index[0], b_pair.index[0]])\n",
    "            pair_output = [a_pair[\"text\"].tolist()[0], b_pair[\"text\"].tolist()[0], 0]\n",
    "            sts_emoji_pair_list.append(pair_output)\n",
    "            a = a + 1\n",
    "            #print(a)\n",
    "\n",
    "\n",
    "def make_group_pair(label_num, data_num, exculsive_group, seed=2022):\n",
    "    global emoji_valid\n",
    "    # 3. 각기 다른 레이블 0으로 지정\n",
    "    all_group = list(range(20))\n",
    "    for a_label, b_label in list(combinations(all_group, 2)): # 20C2 = 190개의 조합\n",
    "        for _ in range(data_num):\n",
    "            a_pair = emoji_valid[emoji_valid['label']==a_label].sample(1, random_state=seed)\n",
    "            b_pair = emoji_valid[emoji_valid['label']==b_label].sample(1, random_state=seed)\n",
    "            \n",
    "            emoji_valid = emoji_valid.drop([a_pair.index[0], b_pair.index[0]])\n",
    "            pair_output = [a_pair[\"text\"].tolist()[0], b_pair[\"text\"].tolist()[0], 0]\n",
    "            sts_emoji_pair_list.append(pair_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2->4, 3->4, 4->4, 5->?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성데이터갯수: 170  남은데이터: 761\n",
      "1    221\n",
      "3    211\n",
      "2    161\n",
      "4     97\n",
      "0     71\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "emoji_valid = emoji_dataset['validation'].to_pandas() # 데이터불러오기 #함수실행시 데이터 줄어듬\n",
    "# print(emoji_valid['label'].value_counts())\n",
    "sts_emoji_pair_list = [] # 담을 리스트\n",
    "\n",
    "make_same_pair(num_label, int(smallest_label_num//4)) # 20 * 300\n",
    "print(\"생성데이터갯수:\", len(sts_emoji_pair_list), \" 남은데이터:\", emoji_valid['label'].count())\n",
    "print(emoji_valid['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 10\n"
     ]
    }
   ],
   "source": [
    "from math import comb\n",
    "remain_label_num = sorted(emoji_valid['label'].value_counts().tolist())[0]\n",
    "remain_label_num\n",
    "each_iter_generated_num = comb(num_label,2)\n",
    "print(remain_label_num, each_iter_generated_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성데이터갯수: 340  남은데이터: 421\n",
      "1    153\n",
      "3    143\n",
      "2     93\n",
      "4     29\n",
      "0      3\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 계산식이 이상해보이겠지만. 최대한 레이블갯수가 비슷하도록 생성함\n",
    "# # 최대생성가능갯수: 가장작은 레이블의 데이터 갯수 / (레이블갯수 Combination 2)\n",
    "# 2->1 3->0.75 4->0.5, 5->0.4\n",
    "\n",
    "make_another_pair(num_label, int(remain_label_num // (each_iter_generated_num*2/num_label))) \n",
    "print(\"생성데이터갯수:\", len(sts_emoji_pair_list), \" 남은데이터:\", emoji_valid['label'].count())\n",
    "print(emoji_valid['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this piece of channel 5 grade trash is , quite...</td>\n",
       "      <td>what is 100 % missing here is a script of even...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>so unremittingly awful that labeling it a dog ...</td>\n",
       "      <td>the humor is n't as sharp , the effects not as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i do n't think i laughed out loud once .</td>\n",
       "      <td>the affectionate loopiness that once seemed co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes , dull .</td>\n",
       "      <td>taylor appears to have blown his entire budget...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>green might want to hang onto that ski mask , ...</td>\n",
       "      <td>so devoid of any kind of intelligible story th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>a movie that reminds us of just how exciting a...</td>\n",
       "      <td>one of those energetic surprises , an original...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>zhang ... has done an amazing job of getting r...</td>\n",
       "      <td>the film 's performances are thrilling .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>the sort of film that makes me miss hitchcock ...</td>\n",
       "      <td>it inspires a continuing and deeply satisfying...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>the jabs it employs are short , carefully plac...</td>\n",
       "      <td>altogether , this is successful as a film , wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>collateral damage finally delivers the goods f...</td>\n",
       "      <td>a fast , funny , highly enjoyable movie .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text1  \\\n",
       "0    this piece of channel 5 grade trash is , quite...   \n",
       "1    so unremittingly awful that labeling it a dog ...   \n",
       "2             i do n't think i laughed out loud once .   \n",
       "3                                         yes , dull .   \n",
       "4    green might want to hang onto that ski mask , ...   \n",
       "..                                                 ...   \n",
       "335  a movie that reminds us of just how exciting a...   \n",
       "336  zhang ... has done an amazing job of getting r...   \n",
       "337  the sort of film that makes me miss hitchcock ...   \n",
       "338  the jabs it employs are short , carefully plac...   \n",
       "339  collateral damage finally delivers the goods f...   \n",
       "\n",
       "                                                 text2  label  \n",
       "0    what is 100 % missing here is a script of even...      1  \n",
       "1    the humor is n't as sharp , the effects not as...      1  \n",
       "2    the affectionate loopiness that once seemed co...      1  \n",
       "3    taylor appears to have blown his entire budget...      1  \n",
       "4    so devoid of any kind of intelligible story th...      1  \n",
       "..                                                 ...    ...  \n",
       "335  one of those energetic surprises , an original...      0  \n",
       "336           the film 's performances are thrilling .      0  \n",
       "337  it inspires a continuing and deeply satisfying...      0  \n",
       "338  altogether , this is successful as a film , wh...      0  \n",
       "339          a fast , funny , highly enjoyable movie .      0  \n",
       "\n",
       "[340 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emoji_pair = pd.DataFrame(sts_emoji_pair_list, columns=[\"text1\", \"text2\", \"label\"])\n",
    "df_emoji_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    170\n",
       "0    170\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emoji_pair['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath =\"/home/uj-user/SenCSE/SentEval/data/downstream/TEMP\"\n",
    "df_emoji_pair.to_csv(f'{fpath}/TWEET.input.{task_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stance last stance 4 reaches then runned.\n",
    "if task_name in stance_list[4]:\n",
    "    stance_list = [\"stance_abortion\",\"stance_atheism\", \"stance_climate\",\"stance_feminist\", \"stance_hillary\"]\n",
    "\n",
    "    df = pd.DataFrame(columns=['text1', 'text2', 'label'])\n",
    "    for stance_name in stance_list:\n",
    "        new_df = pd.read_csv(f'{fpath}/TWEET.input.{stance_name}.csv')\n",
    "        print(new_df['label'].value_counts())\n",
    "        df = pd.concat([df, new_df])\n",
    "\n",
    "    df.to_csv(f'{fpath}/TWEET.input.stance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "456fa7a175a6ab795b9c08634c12a15f53db47df85e2d5c7dd9f5a5f9f51caa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
